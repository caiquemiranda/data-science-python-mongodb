{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Versão da Linguagem Python\n",
    "from platform import python_version\n",
    "print('Versão de Python Neste Jupyter Notebook:', python_version())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradiente descendente (GD) é um algoritmo que minimiza (ou maximiza) funções. Para aplicar, comece em um conjunto inicial de valores de parâmetro de uma função e mova iterativamente em direção a um conjunto de valores de parâmetro que minimize a função. A minimização iterativa é alcançada usando cálculo, dando passos na direção negativa do gradiente da função. O GD é importante porque a otimização é uma grande parte do aprendizado de máquina. Além disso, o GD é fácil de implementar, genérico e eficiente (rápido)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Function Minimization (and Maximization)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GD é um algoritmo de otimização iterativa de 1ª ordem para encontrar o mínimo de uma função f. Uma função pode ser denotada como f ou f(x). Simplesmente, GD encontra o erro mínimo minimizando (ou maximizando) uma função de custo. Uma função de custo é algo que você deseja minimizar."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos começar com um exemplo de minimização. Para encontrar o mínimo local de f, dê passos proporcionais ao negativo do gradiente de f no ponto atual. O gradiente é a derivada (taxa de variação) de f. A única fraqueza de GD é que ele encontra o mínimo local em vez do mínimo para toda a função."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Portanto, a derivada de xn é igual a nxnÿ1. Simplesmente, a derivada é o produto do expoente vezes x com o expoente reduzido em 1. Para minimizar f(x) = x4 – 3x3 + 2 encontre a derivada, que é f'(x) = 4x3 – 9x2 . Assim, o 1º passo é sempre encontrar a derivada f'(x). O 2º passo é plotar a função original para ter uma ideia de sua forma. O terceiro passo é executar o GD. O quarto passo é plotar o mínimo local."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O primeiro exemplo encontra o mínimo local de f(x) e exibe f(x), f'(x), e mínimo na subparcela conforme visto na Figura 4-1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt, numpy as np\n",
    "\n",
    "def f(x):    \n",
    "    return x**4 - 3 * x**3 + 2\n",
    "\n",
    "def df(x):    \n",
    "    return 4 * x**3 - 9 * x**2\n",
    "\n",
    "if __name__ == \"__main__\":    \n",
    "    x = np.arange(-5, 5, 0.2) \n",
    "    y, y_dx = f(x), df(x) \n",
    "    f, axarr = plt.subplots(3, sharex=True)\n",
    "\n",
    "    axarr[0].plot(x, y, color='mediumspringgreen') \n",
    "    axarr[0].set_xlabel('x') \n",
    "    axarr[0].set_ylabel('f(x)') \n",
    "    axarr[0].set_title('f(x)')   \n",
    "    \n",
    "    axarr[1].plot(x, y_dx, color='coral') \n",
    "    axarr[1].set_xlabel('x')  \n",
    "    axarr[1].set_ylabel('dy/dx(x)')   \n",
    "    axarr[1].set_title('derivative of f(x)')   \n",
    "    \n",
    "    axarr[2].set_xlabel('x') \n",
    "    axarr[2].set_ylabel('GD')\n",
    "    axarr[2].set_title('local minimum') \n",
    "\n",
    "    iterations, cur_x, gamma, precision = 0, 6, 0.01, 0.00001  \n",
    "    previous_step_size = cur_x \n",
    "\n",
    "    while previous_step_size > precision: \n",
    "        prev_x = cur_x   \n",
    "        cur_x += -gamma * df(prev_x)   \n",
    "        previous_step_size = abs(cur_x - prev_x)     \n",
    "        iterations += 1 \n",
    "        axarr[2].plot(prev_x, cur_x, \"o\")  \n",
    "        \n",
    "    f.subplots_adjust(hspace=0.3)\n",
    "    f.tight_layout()   \n",
    "    plt.show()  \n",
    "    print ('minimum:', cur_x, '\\niterations:', iterations)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O exemplo de código começa importando matplotlib e numpy. Ele continua com a função f(x) usada para plotar a função original e a função df(x) usada para plotar a derivada. O bloco principal começa criando valores para f(x). Ele continua criando uma subtrama. GD começa inicializando variáveis. A variável cur_x é o ponto de partida para a simulação. Gama variável é o tamanho do passo. A precisão variável é a tolerância. Tolerância menor se traduz em mais precisão, mas requer mais iterações (recursos). A simulação continua até o tamanho_do_passo_anterior ser maior que a precisão. Cada iteração multiplica -gamma (step_size) pelo gradiente (derivado) no ponto atual para movê-lo para o mínimo local."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variável previous_step_size recebe então a diferença entre cur_x e prev_x. Cada ponto é plotado. O mínimo para f(x) resolvendo para x é aproximadamente 2,25. Sei que esse resultado está correto porque o calculei manualmente. Confira http://www.dummies.com/education/math/calculus/ how-to-find-localextrema-with-the-first-derivative-test/ para boa lição sobre como calcular manualmente."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O segundo exemplo encontra o mínimo e máximo local de f(x) = x3 – 6x2 + 9x + 15. Primeiro encontre f'(x), que é 3x2 – 12x + 9. Em seguida, encontre o mínimo local, gráfico, máximo local , e enredo. Não uso uma subtrama neste caso porque a visualização não é tão rica. Ou seja, é muito mais fácil ver o mínimo e o máximo locais aproximados observando um gráfico de f(x) e mais fácil ver como o processo GD faz sua mágica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt, numpy as np\n",
    "\n",
    "def f(x):   \n",
    "    return x**3 - 6 * x**2 + 9 * x + 15\n",
    "\n",
    "def df(x):   \n",
    "    return 3 * x**2 - 12 * x + 9\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    x = np.arange(-0.5, 5, 0.2) \n",
    "    y = f(x)    \n",
    "    \n",
    "    plt.figure('f(x)')\n",
    "    plt.xlabel('x') \n",
    "    plt.ylabel('f(x)')   \n",
    "    plt.title('f(x)')\n",
    "    plt.plot(x, y, color='blueviolet')    \n",
    "    plt.figure('local minimum') \n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('GD')\n",
    "    plt.title('local minimum') \n",
    "    \n",
    "    iterations, cur_x, gamma, precision = 0, 6, 0.01, 0.00001   \n",
    "    previous_step_size = cur_x   \n",
    "    \n",
    "    while previous_step_size > precision:    \n",
    "        prev_x = cur_x       \n",
    "        cur_x += -gamma * df(prev_x)  \n",
    "        previous_step_size = abs(cur_x - prev_x)     \n",
    "        iterations += 1   \n",
    "        plt.plot(prev_x, cur_x, \"o\") \n",
    "        \n",
    "    local_min = cur_x \n",
    "    print ('minimum:', local_min, 'iterations:', iterations)   \n",
    "    \n",
    "    plt.figure('local maximum') \n",
    "    plt.xlabel('x')   \n",
    "    plt.ylabel('GD') \n",
    "    plt.title('local maximum')  \n",
    "    \n",
    "    iterations, cur_x, gamma, precision = 0, 0.5, 0.01, 0.00001  \n",
    "    previous_step_size = cur_x\n",
    "        \n",
    "    while previous_step_size > precision:\n",
    "        prev_x = cur_x   \n",
    "        cur_x += -gamma * -df(prev_x)   \n",
    "        previous_step_size = abs(cur_x - prev_x)    \n",
    "        iterations += 1    \n",
    "        plt.plot(prev_x, cur_x, \"o\")   \n",
    "    \n",
    "    local_max = cur_x   \n",
    "    print ('maximum:', local_max, 'iterations:', iterations)   \n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código começa importando as bibliotecas matplotlib e numpy. Ele continua com as funções f(x) e df(x), que representam a função original e sua derivada algoritmicamente. O bloco principal começa criando dados para f(x) e plotando-os. Ele continua encontrando o mínimo e o máximo locais e plotando-os. Observe que cur_x (o ponto inicial) para mínimo local é 6, enquanto é 0,5 para máximo local."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É aqui que a ciência de dados é mais uma arte do que uma ciência, porque encontrei esses pontos por tentativa e erro. Observe também que GD para o máximo local é a negação da derivada. Mais uma vez, sei que os resultados estão corretos porque calculei manualmente o mínimo e o máximo locais. A principal razão pela qual usei plotagens separadas em vez de uma subparcela para este exemplo é demonstrar por que é tão importante plotar f(x). Apenas olhando para o gráfico, você pode dizer que o máximo local de x para f(x) está próximo de um, e o mínimo local de x para f(x) está próximo de 3. Além disso, você pode ver que o função tem um máximo geral que é maior que 1 deste gráfico. As Figuras 4-2, 4-3 e 4-4 fornecem as visualizações."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimização da Função Sigmoide (e Maximização)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma função sigmóide é uma função matemática com uma curva em forma de S ou sigmóide. É muito importante na ciência de dados por vários motivos. Primeiro, é facilmente diferenciável em relação aos parâmetros de rede, que são fundamentais no treinamento de redes neurais. Em segundo lugar, as funções de distribuição cumulativas para muitas distribuições de probabilidade comuns são sigmoidais. Em terceiro lugar, muitos processos naturais (por exemplo, curvas de aprendizado complexas) seguem uma curva sigmoidal ao longo do tempo. Portanto, uma função sigmóide é frequentemente usada se nenhum modelo matemático específico estiver disponível."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O primeiro exemplo encontra o mínimo local da função sigmoide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt, numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def df(x):\n",
    "    return x * (1-x)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    x = np.arange(-10., 10., 0.2)\n",
    "    y, y_dx = sigmoid(x), df(x)\n",
    "    f, axarr = plt.subplots(3, sharex=True)\n",
    "    axarr[0].plot(x, y, color='lime')\n",
    "    axarr[0].set_xlabel('x')\n",
    "    axarr[0].set_ylabel('f(x)')\n",
    "    axarr[0].set_title('Sigmoid Function')\n",
    "    axarr[1].plot(x, y_dx, color='coral')\n",
    "    axarr[1].set_xlabel('x')\n",
    "    axarr[1].set_ylabel('dy/dx(x)')\n",
    "    axarr[1].set_title('Derivative of f(x)')\n",
    "    axarr[2].set_xlabel('x')\n",
    "    axarr[2].set_ylabel('GD')\n",
    "    axarr[2].set_title('local minimum')\n",
    "    iterations, cur_x, gamma, precision = 0, 0.01, 0.01, 0.00001\n",
    "    previous_step_size = cur_x\n",
    "    \n",
    "    while previous_step_size > precision:\n",
    "        prev_x = cur_x\n",
    "        cur_x += -gamma * df(prev_x)\n",
    "        previous_step_size = abs(cur_x - prev_x)\n",
    "        iterations += 1\n",
    "        plt.plot(prev_x, cur_x, \"o\")\n",
    "    \n",
    "    f.subplots_adjust(hspace=0.3)\n",
    "    f.tight_layout()\n",
    "    print ('minimum:', cur_x, '\\niterations:', iterations)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código começa importando matplotlib e numpy. Ele continua com as funções sigmoid(x) e df(x), que representam a função sigmoid e sua derivada algoritmicamente. O bloco principal começa criando dados para f(x) ef'(x). Ele continua criando subparcelas para f(x), f'(x) e o mínimo local. Nesse caso, o uso de subtramas foi bom para visualização. É fácil ver pelos gráficos f(x) e f'(x) (Figura 4-5) que o mínimo local está próximo de 0. Em seguida, o código executa GD para encontrar o mínimo local e plota-o."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Novamente, o ponto de partida para GD, cur_x, foi encontrado por tentativa e erro. Se você iniciar cur_x mais longe do mínimo local (você pode estimar isso observando a subtrama de f'(x)), o número de iterações aumenta porque leva mais tempo para o algoritmo GD convergir para o mínimo local. Como esperado, o mínimo local é aproximadamente 0."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O segundo exemplo encontra o máximo local da função sigmoide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt, numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "def df(x):\n",
    "    return x * (1-x)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    x = np.arange(-10., 10., 0.2)\n",
    "    y, y_dx = sigmoid(x), df(x)\n",
    "    f, axarr = plt.subplots(3, sharex=True)\n",
    "    axarr[0].plot(x, y, color='lime')\n",
    "    axarr[0].set_xlabel('x')\n",
    "    axarr[0].set_ylabel('f(x)')\n",
    "    axarr[0].set_title('Sigmoid Function')\n",
    "    axarr[1].plot(x, y_dx, color='coral')\n",
    "    axarr[1].set_xlabel('x')\n",
    "    axarr[1].set_ylabel('dy/dx(x)')\n",
    "    axarr[1].set_title('Derivative of f(x)')\n",
    "    axarr[2].set_xlabel('x')\n",
    "    axarr[2].set_ylabel('GD')\n",
    "    axarr[2].set_title('local maximum')\n",
    "    iterations, cur_x, gamma, precision = 0, 0.01, 0.01, 0.00001\n",
    "    previous_step_size = cur_x\n",
    "\n",
    "    while previous_step_size > precision:\n",
    "        prev_x = cur_x\n",
    "        cur_x += -gamma * -df(prev_x)\n",
    "        previous_step_size = abs(cur_x - prev_x)\n",
    "        iterations += 1\n",
    "        plt.plot(prev_x, cur_x, \"o\")\n",
    "    f.subplots_adjust(hspace=0.3)\n",
    "    f.tight_layout()\n",
    "    print ('maximum:', cur_x, '\\niterations:', iterations)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código começa importando matplotlib e numpy. Ele continua com as funções sigmoid(x) e df(x), que representam a função sigmoid e sua derivada algoritmicamente. O bloco principal começa criando dados para f(x) ef'(x). Ele continua criando subparcelas para f(x), f'(x) e o máximo local (Figura 4-6). É fácil ver no gráfico de f(x) que o máximo local está próximo de 1. Em seguida, o código executa GD para encontrar o máximo local e plotá-lo. Novamente, o ponto de partida para GD, cur_x, foi encontrado por tentativa e erro. Se você iniciar cur_x mais longe do máximo local (você pode estimar isso observando a subtrama de f(x)), o número de iterações aumenta porque leva mais tempo para o algoritmo GD convergir para o máximo local. Como esperado, o máximo local é aproximadamente 1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimização da Distância Euclidiana Controlando o tamanho do passo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A distância euclidiana é a distância em linha reta comum entre dois pontos no espaço euclidiano. Com esta distância, o espaço euclidiano torna-se um espaço métrico. A norma associada é a norma euclidiana (EN). A EN atribui a cada vetor o comprimento de sua seta. Então, EN é realmente apenas a magnitude de um vetor. Um espaço vetorial no qual uma norma é definida é o espaço vetorial normado."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para encontrar o mínimo local de f(x) no espaço tridimensional (3-D), o primeiro passo é encontrar o mínimo para todos os vetores 3-D. O segundo passo é criar um vetor aleatório 3-D [x, y, z]. O terceiro passo é escolher um ponto de partida aleatório e, em seguida, dar pequenos passos na direção oposta do gradiente f'(x) até chegar a um ponto onde o gradiente é muito pequeno. Cada pequeno passo (do vetor atual para o próximo vetor) é medido com a métrica ED. A métrica ED é a distância entre dois pontos no espaço euclidiano. A métrica é necessária porque precisamos saber como nos mover a cada pequeno passo. Assim, a métrica ED complementa GD para encontrar o mínimo local no espaço 3-D."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O exemplo de código localiza o mínimo local da função sigmoide em espaço 3-D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random, numpy as np\n",
    "from scipy.spatial import distance\n",
    "\n",
    "def step(v, direction, step_size): \n",
    "    return [v_i + step_size * direction_i for v_i, direction_i in zip(v, direction)]\n",
    "\n",
    "def sigmoid_gradient(v):  \n",
    "    return [v_i * (1-v_i) for v_i in v]\n",
    "\n",
    "def mod_vector(v):   \n",
    "    for i, v_i in enumerate(v):\n",
    "    \n",
    "        if v_i == float(\"inf\") or v_i == float(\"-inf\"):\n",
    "            v[i] = random.randint(-1, 1)   \n",
    "    \n",
    "    return v\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    v = [random.randint(-10, 10) for i in range(3)]\n",
    "    tolerance = 0.0000001\n",
    "    iterations = 1  \n",
    "    fig = plt.figure('Euclidean')  \n",
    "    ax = fig.add_subplot(111, projection='3d')  \n",
    "    while True:  \n",
    "        gradient = sigmoid_gradient(v)    \n",
    "        next_v = step(v, gradient, -0.01)    \n",
    "        \n",
    "        xs = gradient[0]\n",
    "        ys = gradient[1]  \n",
    "        zs = gradient[2]   \n",
    "        \n",
    "        ax.scatter(xs, ys, zs, c='lime', marker='o')\n",
    "        v = mod_vector(v)    \n",
    "        next_v = mod_vector(next_v)  \n",
    "        test_v = distance.euclidean(v, next_v) \n",
    "\n",
    "        if test_v < tolerance:   \n",
    "            break        \n",
    "        v = next_v   \n",
    "        iterations += 1 \n",
    "\n",
    "    print ('minimum:', test_v, '\\niterations:', iterations)   \n",
    "    ax.set_xlabel('X axis')\n",
    "    ax.set_ylabel('Y axis')  \n",
    "    ax.set_zlabel('Z axis') \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código começa importando as bibliotecas matplotlib, mpl_toolkits, random, numpy e scipy. A função step() move um vetor em uma direção (com base no gradiente), por um tamanho de passo. A função sigmoid_gradient() é o f'(sigmoid) retornado como um ponto no espaço 3-D. A função mod_vector() garante que um vetor errado gerado pela simulação seja tratado corretamente. O bloco principal começa criando um vetor 3-D gerado aleatoriamente [x, y, z] como ponto de partida para a simulação. Continua criando uma tolerância (precisão). Uma tolerância menor resulta em um resultado mais preciso. Uma subtrama é criada para manter uma renderização 3-D do mínimo local (Figura 4-7). A simulação GD cria um conjunto de vetores 3-D influenciados pelo gradiente sigmoide até que o gradiente seja muito pequeno. O tamanho (magnitude) do gradiente é calculado pela métrica ED. O mínimo local, como esperado, é próximo de 0."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estabilizando a Minimização da Distância Euclidiana com a Simulação de Monte Carlo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O experimento da distância euclidiana no exemplo anterior é ancorado por um processo estocástico. Ou seja, o vetor inicial v é gerado estocasticamente por randomint(). Como resultado, cada execução do experimento GD gera um resultado diferente para o número de iterações. Do Capítulo 2, já sabemos que a simulação de Monte Carlo (MCS) modela com eficiência processos estocásticos (aleatórios). No entanto, o MCS também pode estabilizar experimentos estocásticos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O exemplo de código primeiro envolve o experimento GD em um loop que é executado n número de simulações. Com n simulações, um número médio de iterações é calculado. O código resultante é então agrupado em outro loop que executa m tentativas. Com m tentativas, é calculado um intervalo médio entre cada número médio de iterações. O intervalo é calculado subtraindo o mínimo da iteração média máxima. Quanto menor a lacuna, mais estável (preciso) o resultado. Para aumentar a precisão, aumente simulações (n). A única limitação é o poder de computação. Ou seja, executar 1.000 simulações exige muito mais poder de computação do que 100. Resultados estáveis (precisos) permitem a comparação com experimentos alternativos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, numpy as np\n",
    "from scipy.spatial import distance\n",
    "\n",
    "def step(v, direction, step_size):\n",
    "    return [v_i + step_size * direction_i \n",
    "            for v_i, direction_i in zip(v, direction)]\n",
    "    \n",
    "def sigmoid_gradient(v):   \n",
    "    return [v_i * (1-v_i) for v_i in v]\n",
    "\n",
    "def mod_vector(v):\n",
    "    for i, v_i in enumerate(v):\n",
    "        if v_i == float(\"inf\") or v_i == float(\"-inf\"):\n",
    "            v[i] = random.randint(-1, 1)\n",
    "    return v\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    trials= 10   \n",
    "    sims = 10  \n",
    "    avg_its = []   \n",
    "    for _ in range(trials):   \n",
    "        its = [] \n",
    "        for _ in range(sims):   \n",
    "            v = [random.randint(-10, 10) for i in range(3)] \n",
    "            tolerance = 0.0000001 \n",
    "            iterations = 0     \n",
    "            while True: \n",
    "                gradient = sigmoid_gradient(v)  \n",
    "                next_v = step(v, gradient, -0.01)    \n",
    "                v = mod_vector(v)\n",
    "                next_v = mod_vector(next_v) \n",
    "                test_v = distance.euclidean(v, next_v) \n",
    "                \n",
    "                if test_v < tolerance:\n",
    "                    break\n",
    "                \n",
    "                v = next_v   \n",
    "                iterations += 1\n",
    "            its.append(iterations) \n",
    "        a = round(np.mean(its))\n",
    "        avg_its.append(a) \n",
    "    gap = np.max(avg_its) - np.min(avg_its)   \n",
    "    print (trials, 'trials with', sims, 'simulations each:')   \n",
    "    print ('gap', gap)  \n",
    "    print ('avg iterations', round(np.mean(avg_its)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A saída é para 10, 100 e 1.000 simulações. Executando 1.000 simulações dez vezes (tentativas), a diferença cai para 13. Portanto, há alta confiança de que o número de iterações necessárias para minimizar a função está próximo de 1.089. Podemos estabilizar ainda mais agrupando o código em outro loop para diminuir a variação no intervalo e no número de iterações. No entanto, o tempo de processamento do computador torna-se um problema. Aproveitar o MCS para esse tipo de experimento é um forte argumento para a computação em nuvem. Pode ser difícil entender esse aplicativo de MCS, mas é uma ferramenta muito poderosa para trabalhar e resolver problemas de ciência de dados."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Substituindo um método NumPy para acelerar Minimização da Distância Euclidiana"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como as matrizes numpy são mais rápidas que as listas Python, segue-se que usar um método numpy seria mais eficiente para calcular a distância euclidiana. O exemplo de código substitui np.linalg.norm() por distance.euclidean() para calcular a distância euclidiana para o experimento GD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random, numpy as np\n",
    "\n",
    "def step(v, direction, step_size):\n",
    "    return [v_i + step_size * direction_i   \n",
    "            for v_i, direction_i in zip(v, direction)]\n",
    "\n",
    "def sigmoid_gradient(v):\n",
    "    return [v_i * (1-v_i) for v_i in v]\n",
    "\n",
    "def round_v(v):    \n",
    "    return np.round(v, decimals=3)\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    v = [random.randint(-10, 10) for i in range(3)]    \n",
    "    tolerance = 0.0000001   \n",
    "    iterations = 1   \n",
    "    fig = plt.figure('norm')  \n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    while True:   \n",
    "        gradient = sigmoid_gradient(v) \n",
    "        next_v = step(v, gradient, -0.01)      \n",
    "        round_gradient = round_v(gradient)\n",
    "        \n",
    "        xs = round_gradient[0]   \n",
    "        ys = round_gradient[1]\n",
    "        zs = round_gradient[2] \n",
    "        \n",
    "        ax.scatter(xs, ys, zs, c='lime', marker='o')\n",
    "        norm_v = np.linalg.norm(v)\n",
    "        norm_next_v = np.linalg.norm(next_v)\n",
    "        test_v = norm_v - norm_next_v   \n",
    "    \n",
    "        if test_v < tolerance:  \n",
    "            break \n",
    "    \n",
    "        v = next_v  \n",
    "        iterations += 1 \n",
    "    \n",
    "    print ('minimum:', test_v, '\\niterations:', iterations)\n",
    "    \n",
    "    ax.set_xlabel('X axis')\n",
    "    ax.set_ylabel('Y axis')   \n",
    "    ax.set_zlabel('Z axis')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O número de iterações é muito menor em 31 (Figura 4-8). No entanto, dado que o experimento GD é estocástico, podemos usar MCS para comparação objetiva. Usando a mesma metodologia MCS, o exemplo de código primeiro envolve o experimento GD em um loop que executa n número de simulações. O código resultante é então agrupado em outro loop que executa m tentativas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, numpy as np\n",
    "\n",
    "def step(v, direction, step_size):\n",
    "    return [v_i + step_size * direction_i\n",
    "            for v_i, direction_i in zip(v, direction)]\n",
    "\n",
    "def sigmoid_gradient(v):  \n",
    "    return [v_i * (1-v_i) for v_i in v]\n",
    "\n",
    "def round_v(v):\n",
    "    return np.round(v, decimals=3)\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    trials = 10    \n",
    "    sims = 10\n",
    "    avg_its = []  \n",
    "    for _ in range(trials):\n",
    "        its = []    \n",
    "        for _ in range(sims):  \n",
    "            v = [random.randint(-10, 10) for i in range(3)] \n",
    "            tolerance = 0.0000001 \n",
    "            iterations = 0     \n",
    "            while True:  \n",
    "                gradient = sigmoid_gradient(v)\n",
    "                next_v = step(v, gradient, -0.01)\n",
    "                norm_v = np.linalg.norm(v) \n",
    "                norm_next_v = np.linalg.norm(next_v)\n",
    "                test_v = norm_v - norm_next_v   \n",
    "                \n",
    "                if test_v < tolerance:  \n",
    "                    break  \n",
    "                \n",
    "                v = next_v  \n",
    "                iterations += 1      \n",
    "            its.append(iterations)\n",
    "        a = round(np.mean(its))\n",
    "        avg_its.append(a) \n",
    "    gap = np.max(avg_its) - np.min(avg_its)  \n",
    "    print (trials, 'trials with', sims, 'simulations each:') \n",
    "    print ('gap', gap)\n",
    "    print ('avg iterations', round(np.mean(avg_its)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O processamento é muito mais rápido usando numpy. O número médio de iterações é próximo a 193. Como tal, usar a alternativa numpy para calcular a distância euclidiana é mais de cinco vezes mais rápido!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimização e maximização estocástica do gradiente descendente"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Até este ponto do capítulo, experimentos de otimização usaram lote GD. O lote GD calcula o gradiente usando todo o conjunto de dados. O GD estocástico calcula o gradiente usando uma única amostra, por isso é computacionalmente muito mais rapido. É chamado de GD estocástico porque o gradiente é determinado aleatoriamente. No entanto, ao contrário do lote GD, o estocástico GD é uma aproximação. Se o gradiente exato for necessário, o GD estocástico não é ideal. Outro problema com o GD estocástico é que ele pode pairar em torno do mínimo para sempre sem realmente convergir. Portanto, é importante plotar o progresso da simulação para ver o que está acontecendo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos mudar de direção e otimizar outra função importante — soma residual dos quadrados (RSS). Uma função RSS é uma técnica estatística que mede a quantidade de erro (variância) restante entre a função de regressão e o conjunto de dados. A análise de regressão é um algoritmo que estima relações entre variáveis. É amplamente utilizado para previsão e previsão. É também um algoritmo popular de modelagem e previsão para aplicações de ciência de dados."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O primeiro exemplo de código gera uma amostra, executa o experimento GD n vezes e processa a amostra aleatoriamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random, numpy as np\n",
    "\n",
    "def rnd():    \n",
    "        return [random.randint(-10,10) for i in range(3)]\n",
    "\n",
    "def random_vectors(n): \n",
    "        ls = []   \n",
    "        for v in range(n): \n",
    "                ls.append(rnd())  \n",
    "\n",
    "        return ls\n",
    "        \n",
    "def sos(v):\n",
    "        return sum(v_i ** 2 for v_i in v)\n",
    "\n",
    "def sos_gradient(v):    \n",
    "        return [2 * v_i for v_i in v]\n",
    "\n",
    "def in_random_order(data):   \n",
    "        indexes = [i for i, _ in enumerate(data)]\n",
    "        random.shuffle(indexes)   \n",
    "        for i in indexes:\n",
    "                yield data[i]\n",
    "\n",
    "if __name__ == \"__main__\":   \n",
    "        \n",
    "        v, x, y = rnd(), random_vectors(3), random_vectors(3) \n",
    "        data = list(zip(x, y)) \n",
    "        theta = v    \n",
    "        alpha, value = 0.01, 0 \n",
    "        min_theta, min_value = None, float(\"inf\")    \n",
    "        iterations_with_no_improvement = 0  \n",
    "        n, x = 30, 1 \n",
    "        for i, _ in enumerate(range(n)):\n",
    "                y = np.linalg.norm(theta)\n",
    "                plt.scatter(x, y, c='r')  \n",
    "                x = x + 1  \n",
    "                s = []     \n",
    "                for x_i, y_i in data:  \n",
    "                        s.extend([sos(theta), sos(x_i), sos(y_i)])   \n",
    "                \n",
    "                value = sum(s)\n",
    "                \n",
    "                if value < min_value:  \n",
    "                        min_theta, min_value = theta, value  \n",
    "                        iterations_with_no_improvement = 0      \n",
    "                        alpha = 0.01   \n",
    "                else:\n",
    "                        iterations_with_no_improvement += 1 \n",
    "                        alpha *= 0.9  \n",
    "                \n",
    "                g = []\n",
    "                \n",
    "                for x_i, y_i in in_random_order(data):\n",
    "                        g.extend([sos_gradient(theta), sos_gradient(x_i),       \n",
    "                                        sos_gradient(y_i)]) \n",
    "                        for v in g:   \n",
    "                                theta = np.around(np.subtract(theta,alpha*np.array(v)),3)  \n",
    "                        g = []\n",
    "                \n",
    "        print ('minimum:', np.around(min_theta, 4), \n",
    "                'with', i+1, 'iterations') \n",
    "        \n",
    "        print ('iterations with no improvement:',\n",
    "                iterations_with_no_improvement)\n",
    "                \n",
    "        print ('magnitude of min vector:', np.linalg.norm(min_theta))   \n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código começa importando matplotlib, random e numpy. Ela continua com a função rnd(), que retorna uma lista de inteiros aleatórios de –10 a 10. A função random_vectors() gera uma lista (amostra aleatória) de n números. A função sos() retorna o RSS para um vetor. A função sos_ gradient() retorna a derivada (gradiente) de RSS para um vetor. A função in_random_order() gera uma lista de índices aleatoriamente embaralhados. Esta função adiciona o sabor estocástico ao algoritmo GD. O bloco principal começa gerando um vetor aleatório v como ponto de partida para a simulação. Ele continua criando uma amostra de vetores x e y de tamanho 3. Em seguida, o vetor é atribuído a theta, que é um nome comum para um vetor de alguma distribuição de probabilidade geral. Podemos chamar o vetor do que quisermos, mas um problema comum da ciência de dados é encontrar o(s) valor(es) de teta. O código continua com um alfa de tamanho de etapa fixo, valor teta mínimo, valor final mínimo, iterações sem melhoria, número de simulações n e um valor de gráfico para a coordenada x (Figura 4-9)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simulação começa atribuindo a y a magnitude de teta. Em seguida, plota as coordenadas x e y atuais. A coordenada x é incrementada em 1 para plotar a convergência para o mínimo para cada coordenada y. O próximo bloco de código localiza o RSS para cada teta e a amostra dos valores x e y. Este valor determina se a simulação está pairando em torno do mínimo local em vez de convergir. A parte final do código percorre os pontos de dados de amostra em ordem aleatória (estocástica), encontra o gradiente de teta, x e y, coloca esses três valores na lista g e percorre esse vetor para encontrar o próximo valor teta."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso não é simples, mas é assim que o GD estocástico opera. Observe que o mínimo gerado é 2,87, que não é o mínimo verdadeiro de 0. Portanto, GD estocástico requer poucas iterações, mas não produz o mínimo verdadeiro."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simulação anterior pode ser refinada ajustando o algoritmo para encontrar o próximo teta. No exemplo anterior, o próximo teta é calculado para o gradiente com base no teta atual, valor x e valor y para cada amostra. No entanto, o novo teta real é baseado no terceiro ponto de dados no amostra. Assim, o 2º exemplo é refinado tomando o teta mínimo de toda a amostra em vez do 3º ponto de dados: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random, numpy as np\n",
    "\n",
    "def rnd(): \n",
    "    return [random.randint(-10,10) for i in range(3)]\n",
    "\n",
    "def random_vectors(n): \n",
    "    ls = []  \n",
    "    for v in range(n):\n",
    "        ls.append([random.randint(-10,10) for i in range(3)])\n",
    "    \n",
    "    return ls\n",
    "\n",
    "def sos(v): \n",
    "    return sum(v_i ** 2 for v_i in v)\n",
    "\n",
    "def sos_gradient(v):\n",
    "    return [2 * v_i for v_i in v]\n",
    "\n",
    "def in_random_order(data):   \n",
    "    indexes = [i for i, _ in enumerate(data)]\n",
    "    random.shuffle(indexes)  \n",
    "    for i in indexes:\n",
    "        yield data[i]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    v, x, y = rnd(), random_vectors(3), random_vectors(3)  \n",
    "    data = list(zip(x, y))  \n",
    "    theta = v    \n",
    "    alpha, value = 0.01, 0 \n",
    "    min_theta, min_value = None, float(\"inf\")  \n",
    "    iterations_with_no_improvement = 0    \n",
    "    n, x = 60, 1\n",
    "    \n",
    "    for i, _ in enumerate(range(n)):   \n",
    "        y = np.linalg.norm(theta)    \n",
    "        plt.scatter(x, y, c='r')   \n",
    "        x = x + 1  \n",
    "        s = []     \n",
    "        \n",
    "        for x_i, y_i in data:   \n",
    "            s.extend([sos(theta), sos(x_i), sos(y_i)])\n",
    "        \n",
    "        value = sum(s)\n",
    "        if value < min_value:   \n",
    "            min_theta, min_value = theta, value  \n",
    "            iterations_with_no_improvement = 0\n",
    "            alpha = 0.01 \n",
    "        else:\n",
    "            iterations_with_no_improvement += 1 \n",
    "            alpha *= 0.9    \n",
    "            \n",
    "        g, t, m = [], [], [] \n",
    "        for x_i, y_i in in_random_order(data):\n",
    "            g.extend([sos_gradient(theta), sos_gradient(x_i),  \n",
    "                        sos_gradient(y_i)])  \n",
    "            \n",
    "            m = np.around([np.linalg.norm(x) for x in g], 2) \n",
    "            for v in g: \n",
    "                theta = np.around(np.subtract(theta,alpha*np.array(v)),3) \n",
    "                t.append(np.around(theta,2)) \n",
    "            \n",
    "            mm = np.argmin(m)  \n",
    "            theta = t[mm]\n",
    "            g, m, t = [], [], [] \n",
    "        \n",
    "    print ('minimum:', np.around(min_theta, 4),\n",
    "            'with', i+1, 'iterations')\n",
    "    \n",
    "    print ('iterations with no improvement:', \n",
    "            iterations_with_no_improvement)\n",
    "        \n",
    "    print ('magnitude of min vector:', np.linalg.norm(min_theta))    \n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A única diferença no código está na parte inferior, onde o teta mínimo é calculado (Figura 4-10). Embora tenha levado 60 iterações, o mínimo é muito mais próximo de 0 e muito mais estável. Ou seja, o exemplo anterior se desvia um pouco mais cada vez que o experimento é executado."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O terceiro exemplo encontra o máximo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random, numpy as np\n",
    "\n",
    "def rnd():  \n",
    "    return [random.randint(-10,10) for i in range(3)]\n",
    "\n",
    "def random_vectors(n):\n",
    "    ls = []\n",
    "      \n",
    "    for v in range(n): \n",
    "        ls.append([random.randint(-10,10) for i in range(3)])  \n",
    "    return ls\n",
    "\n",
    "def sos_gradient(v):  \n",
    "    return [2 * v_i for v_i in v]\n",
    "\n",
    "def negate(function):\n",
    "    def new_function(*args, **kwargs):  \n",
    "        return np.negative(function(*args, **kwargs))\n",
    "    \n",
    "    return new_function\n",
    "\n",
    "def in_random_order(data):\n",
    "    indexes = [i for i, _ in enumerate(data)]\n",
    "    random.shuffle(indexes) \n",
    "    for i in indexes: \n",
    "        yield data[i]\n",
    "        \n",
    "if __name__ == \"__main__\": \n",
    "    v, x, y = rnd(), random_vectors(3), random_vectors(3) \n",
    "    data = list(zip(x, y)) \n",
    "    theta, alpha = v, 0.01  \n",
    "    neg_gradient = negate(sos_gradient)   \n",
    "    n, x = 100, 1\n",
    "    \n",
    "    for i, row in enumerate(range(n)):\n",
    "        y = np.linalg.norm(theta)   \n",
    "        plt.scatter(x, y, c='r')   \n",
    "        x = x + 1    \n",
    "        g = []      \n",
    "        for x_i, y_i in in_random_order(data):\n",
    "            g.extend([neg_gradient(theta), neg_gradient(x_i), \n",
    "                      neg_gradient(y_i)])\n",
    "            for v in g:\n",
    "                theta = np.around(np.subtract(theta,alpha*np.array(v)),3) \n",
    "            g = []  \n",
    "        \n",
    "    print ('maximum:', np.around(theta, 4),  \n",
    "            'with', i+1, 'iterations')\n",
    "    \n",
    "    print ('magnitude of max vector:', np.linalg.norm(theta))  \n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A única diferença no código do 1º exemplo é o negate() função, que nega o gradiente para encontrar o máximo. Como o máximo de RSS é infinito (podemos dizer pela visualização na Figura 4-11), podemos parar em 100 iterações. Tente 1.000 iterações e veja o que acontece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -a \"Caique Miranda\" -gu \"caiquemiranda\" -iv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
