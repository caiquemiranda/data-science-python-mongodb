{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Versão da Linguagem Python\n",
    "from platform import python_version\n",
    "print('Versão de Python Neste Jupyter Notebook:', python_version())\n",
    "\n",
    "# usaremos o filtro 'warning' para deixar mais limpo.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Function Minimization (and Maximization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt, numpy as np\n",
    "\n",
    "def f(x):    \n",
    "    return x**4 - 3 * x**3 + 2\n",
    "\n",
    "def df(x):    \n",
    "    return 4 * x**3 - 9 * x**2\n",
    "\n",
    "if __name__ == \"__main__\":    \n",
    "    x = np.arange(-5, 5, 0.2) \n",
    "    y, y_dx = f(x), df(x) \n",
    "    f, axarr = plt.subplots(3, sharex=True)\n",
    "\n",
    "    axarr[0].plot(x, y, color='mediumspringgreen') \n",
    "    axarr[0].set_xlabel('x') \n",
    "    axarr[0].set_ylabel('f(x)') \n",
    "    axarr[0].set_title('f(x)')   \n",
    "    \n",
    "    axarr[1].plot(x, y_dx, color='coral') \n",
    "    axarr[1].set_xlabel('x')  \n",
    "    axarr[1].set_ylabel('dy/dx(x)')   \n",
    "    axarr[1].set_title('derivative of f(x)')   \n",
    "    \n",
    "    axarr[2].set_xlabel('x') \n",
    "    axarr[2].set_ylabel('GD')\n",
    "    axarr[2].set_title('local minimum') \n",
    "\n",
    "    iterations, cur_x, gamma, precision = 0, 6, 0.01, 0.00001  \n",
    "    previous_step_size = cur_x \n",
    "\n",
    "    while previous_step_size > precision: \n",
    "        prev_x = cur_x   \n",
    "        cur_x += -gamma * df(prev_x)   \n",
    "        previous_step_size = abs(cur_x - prev_x)     \n",
    "        iterations += 1 \n",
    "        axarr[2].plot(prev_x, cur_x, \"o\")  \n",
    "        f.subplots_adjust(hspace=0.3)\n",
    "        f.tight_layout()   \n",
    "        plt.show()  \n",
    "        print ('minimum:', cur_x, '\\niterations:', iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt, numpy as np\n",
    "\n",
    "def f(x):   \n",
    "    return x**3 - 6 * x**2 + 9 * x + 15\n",
    "\n",
    "def df(x):   \n",
    "    return 3 * x**2 - 12 * x + 9\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    x = np.arange(-0.5, 5, 0.2) \n",
    "    y = f(x)    plt.figure('f(x)')\n",
    "    \n",
    "    plt.xlabel('x') \n",
    "    plt.ylabel('f(x)')   \n",
    "    plt.title('f(x)')\n",
    "    plt.plot(x, y, color='blueviolet')    \n",
    "    plt.figure('local minimum') \n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('GD')\n",
    "    plt.title('local minimum') \n",
    "    \n",
    "    iterations, cur_x, gamma, precision = 0, 6, 0.01, 0.00001   \n",
    "    previous_step_size = cur_x   \n",
    "    \n",
    "    while previous_step_size > precision:    \n",
    "        prev_x = cur_x       \n",
    "        cur_x += -gamma * df(prev_x)  \n",
    "        previous_step_size = abs(cur_x - prev_x)     \n",
    "        iterations += 1   \n",
    "        plt.plot(prev_x, cur_x, \"o\") \n",
    "        \n",
    "        local_min = cur_x \n",
    "        print ('minimum:', local_min, 'iterations:', iterations)   \n",
    "        \n",
    "        plt.figure('local maximum') \n",
    "        plt.xlabel('x')   \n",
    "        plt.ylabel('GD') \n",
    "        plt.title('local maximum')  \n",
    "        \n",
    "        iterations, cur_x, gamma, precision = 0, 0.5, 0.01, 0.00001  \n",
    "        previous_step_size = cur_x\n",
    "        \n",
    "    while previous_step_size > precision:\n",
    "        prev_x = cur_x   \n",
    "        cur_x += -gamma * -df(prev_x)   \n",
    "        previous_step_size = abs(cur_x - prev_x)    \n",
    "        iterations += 1    \n",
    "        plt.plot(prev_x, cur_x, \"o\")   \n",
    "        \n",
    "        local_max = cur_x   \n",
    "        print ('maximum:', local_max, 'iterations:', iterations)   \n",
    "        plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean Distance Minimization Controlling for Step Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random, numpy as np\n",
    "from scipy.spatial import distance\n",
    "\n",
    "def step(v, direction, step_size): \n",
    "    return [v_i + step_size * direction_i            for v_i, direction_i in zip(v, direction)]\n",
    "\n",
    "def sigmoid_gradient(v):  \n",
    "    return [v_i * (1-v_i) for v_i in v]\n",
    "\n",
    "def mod_vector(v):   \n",
    "    for i, v_i in enumerate(v):\n",
    "    \n",
    "        if v_i == float(\"inf\") or v_i == float(\"-inf\"):\n",
    "            v[i] = random.randint(-1, 1)   \n",
    "    \n",
    "    return v\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    v = [random.randint(-10, 10) for i in range(3)]\n",
    "    tolerance = 0.0000001\n",
    "    iterations = 1  \n",
    "    fig = plt.figure('Euclidean')  \n",
    "    ax = fig.add_subplot(111, projection='3d')  \n",
    "    while True:  \n",
    "        gradient = sigmoid_gradient(v)    \n",
    "        next_v = step(v, gradient, -0.01)    \n",
    "        \n",
    "        xs = gradient[0]\n",
    "        ys = gradient[1]  \n",
    "        zs = gradient[2]   \n",
    "        \n",
    "        ax.scatter(xs, ys, zs, c='lime', marker='o')\n",
    "        v = mod_vector(v)    \n",
    "        next_v = mod_vector(next_v)  \n",
    "        test_v = distance.euclidean(v, next_v) \n",
    "\n",
    "        if test_v < tolerance:   \n",
    "            break        \n",
    "        v = next_v   \n",
    "        iterations += 1 \n",
    "\n",
    "    print ('minimum:', test_v, '\\niterations:', iterations)   \n",
    "    ax.set_xlabel('X axis')\n",
    "    ax.set_ylabel('Y axis')  \n",
    "    ax.set_zlabel('Z axis') \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stabilizing Euclidean Distance Minimization with Monte Carlo Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, numpy as np\n",
    "from scipy.spatial import distance\n",
    "\n",
    "def step(v, direction, step_size):\n",
    "    return [v_i + step_size * direction_i \n",
    "            for v_i, direction_i in zip(v, direction)]\n",
    "    \n",
    "def sigmoid_gradient(v):   \n",
    "    return [v_i * (1-v_i) for v_i in v]\n",
    "\n",
    "def mod_vector(v):\n",
    "    for i, v_i in enumerate(v):\n",
    "        if v_i == float(\"inf\") or v_i == float(\"-inf\"):\n",
    "            v[i] = random.randint(-1, 1)\n",
    "    return v\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    trials= 10   \n",
    "    sims = 10  \n",
    "    avg_its = []   \n",
    "    for _ in range(trials):   \n",
    "        its = [] \n",
    "        for _ in range(sims):   \n",
    "            v = [random.randint(-10, 10) for i in range(3)] \n",
    "            tolerance = 0.0000001 \n",
    "            iterations = 0     \n",
    "            while True: \n",
    "                gradient = sigmoid_gradient(v)  \n",
    "                next_v = step(v, gradient, -0.01)    \n",
    "                v = mod_vector(v)\n",
    "                next_v = mod_vector(next_v) \n",
    "                test_v = distance.euclidean(v, next_v) \n",
    "                \n",
    "                if test_v < tolerance:\n",
    "                    break\n",
    "                \n",
    "                v = next_v   \n",
    "                iterations += 1\n",
    "            its.append(iterations) \n",
    "        a = round(np.mean(its))\n",
    "        avg_its.append(a) \n",
    "    gap = np.max(avg_its) - np.min(avg_its)   \n",
    "    print (trials, 'trials with', sims, 'simulations each:')   \n",
    "    print ('gap', gap)  \n",
    "    print ('avg iterations', round(np.mean(avg_its)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Substituting a NumPy Method to Hasten Euclidean Distance Minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random, numpy as np\n",
    "\n",
    "def step(v, direction, step_size):\n",
    "    return [v_i + step_size * direction_i   \n",
    "            for v_i, direction_i in zip(v, direction)]\n",
    "\n",
    "def sigmoid_gradient(v):\n",
    "    return [v_i * (1-v_i) for v_i in v]\n",
    "\n",
    "def round_v(v):    \n",
    "    return np.round(v, decimals=3)\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    v = [random.randint(-10, 10) for i in range(3)]    \n",
    "    tolerance = 0.0000001   \n",
    "    iterations = 1   \n",
    "    fig = plt.figure('norm')  \n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    while True:   \n",
    "        gradient = sigmoid_gradient(v) \n",
    "        next_v = step(v, gradient, -0.01)      \n",
    "        round_gradient = round_v(gradient)\n",
    "        \n",
    "        xs = round_gradient[0]   \n",
    "        ys = round_gradient[1]\n",
    "        zs = round_gradient[2] \n",
    "        \n",
    "        ax.scatter(xs, ys, zs, c='lime', marker='o')\n",
    "        norm_v = np.linalg.norm(v)\n",
    "        norm_next_v = np.linalg.norm(next_v)\n",
    "        test_v = norm_v - norm_next_v   \n",
    "    \n",
    "        if test_v < tolerance:  \n",
    "            break \n",
    "    \n",
    "        v = next_v  \n",
    "        iterations += 1 \n",
    "    \n",
    "    print ('minimum:', test_v, '\\niterations:', iterations)\n",
    "    \n",
    "    ax.set_xlabel('X axis')\n",
    "    ax.set_ylabel('Y axis')   \n",
    "    ax.set_zlabel('Z axis')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, numpy as np\n",
    "\n",
    "def step(v, direction, step_size):\n",
    "    return [v_i + step_size * direction_i\n",
    "            for v_i, direction_i in zip(v, direction)]\n",
    "\n",
    "def sigmoid_gradient(v):  \n",
    "    return [v_i * (1-v_i) for v_i in v]\n",
    "\n",
    "def round_v(v):\n",
    "    return np.round(v, decimals=3)\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    trials = 10    \n",
    "    sims = 10\n",
    "    avg_its = []  \n",
    "    for _ in range(trials):\n",
    "        its = []    \n",
    "        for _ in range(sims):  \n",
    "            v = [random.randint(-10, 10) for i in range(3)] \n",
    "            tolerance = 0.0000001 \n",
    "            iterations = 0     \n",
    "            while True:  \n",
    "                gradient = sigmoid_gradient(v)\n",
    "                next_v = step(v, gradient, -0.01)\n",
    "                norm_v = np.linalg.norm(v) \n",
    "                norm_next_v = np.linalg.norm(next_v)\n",
    "                test_v = norm_v - norm_next_v   \n",
    "                \n",
    "                if test_v < tolerance:  \n",
    "                    break  \n",
    "                \n",
    "                v = next_v  \n",
    "                iterations += 1      \n",
    "            its.append(iterations)\n",
    "        a = round(np.mean(its))\n",
    "        avg_its.append(a) \n",
    "    gap = np.max(avg_its) - np.min(avg_its)  \n",
    "    print (trials, 'trials with', sims, 'simulations each:') \n",
    "    print ('gap', gap)\n",
    "    print ('avg iterations', round(np.mean(avg_its)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent Minimization and Maximization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -a \"Caique Miranda\" -gu \"caiquemiranda\" -iv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
