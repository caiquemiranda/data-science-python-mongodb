{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Versão da Linguagem Python\n",
    "from platform import python_version\n",
    "print('Versão de Python Neste Jupyter Notebook:', python_version())\n",
    "\n",
    "# usaremos o filtro 'warning' para deixar mais limpo.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heat Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def read_json(f):\n",
    "    with open(f) as f:\n",
    "\n",
    "        return json.load(f)\n",
    "\n",
    "def verify_keys(d, **kwargs):\n",
    "    data = d[0].items()\n",
    "    k1 = set([tup[0] for tup in data])\n",
    "\n",
    "    s = kwargs.items()\n",
    "    k2 = set([tup[1] for tup in s])\n",
    "\n",
    "    return list(k1.intersection(k2))\n",
    "\n",
    "    \n",
    "def build_ls(k, d):\n",
    "\n",
    "    return [{k: row[k] for k in (keys)} for row in d]\n",
    "\n",
    "def get_rows(d, n):\n",
    "    [print(row) for i, row in enumerate(d) if i < n]\n",
    "\n",
    "def conv_float(d):\n",
    "\n",
    "    return [dict([k, float(v)] for k, v in row.items()) for row in d]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    f= 'data/wrangled.json'\n",
    "    data = read_json(f)\n",
    "    keys = verify_keys(data, \n",
    "                       c1 = 'sale',\n",
    "                       c2 = 'quan',\n",
    "                       c3 = 'disc',\n",
    "                       c4 = 'prof')\n",
    "\n",
    "    heat = build_ls(keys, data)\n",
    "    print ('1st row in \"heat\":')\n",
    "    get_rows(heat, 1)\n",
    "\n",
    "    heat = conv_float(heat)\n",
    "    print ('\\n1st row in \"heat\" converted to float:')\n",
    "    get_rows(heat, 1)\n",
    "\n",
    "    df = pd.DataFrame(heat)\n",
    "\n",
    "    plt.figure()\n",
    "    sns.heatmap(df.corr(), \n",
    "                annot = True,\n",
    "                cmap = 'OrRd')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt, pandas as pd\n",
    "import numpy as np, json, random as rnd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pandas.plotting import parallel_coordinates\n",
    "\n",
    "def read_json(f):\n",
    "    with open(f) as f:\n",
    "\n",
    "        return json.load(f)\n",
    "\n",
    "def unique_features(k, d):\n",
    "\n",
    "    return list(set([dic[k] for dic in d]))\n",
    "\n",
    "def sire_features(k, d):\n",
    "    return [{k: row[k] for k in (k)} for row in d]\n",
    "\n",
    "def sire_numeric(k, d):\n",
    "    s = conv_float(sire_features(k, d))\n",
    "\n",
    "    return s\n",
    "\n",
    "def sire_sample(k, v, d, m):\n",
    "    indices = np.arange(0, len(d), 1)\n",
    "    s = [d[i] for i in indices if d[i][k] == v]\n",
    "    n = len(s)\n",
    "    num_keys = ['sale', 'quan', 'disc', 'prof']\n",
    "    \n",
    "    for i, row in enumerate(s):\n",
    "        for k in num_keys:\n",
    "            row[k] = float(row[k])\n",
    "    \n",
    "    s = rnd_sample(m, len(s), s)\n",
    "    \n",
    "    return (s, n)\n",
    "\n",
    "def rnd_sample(m, n, d):\n",
    "    indices = sorted(rnd.sample(range(n), m))\n",
    "    \n",
    "    return [d[i] for i in indices]\n",
    "\n",
    "def conv_float(d):\n",
    "    \n",
    "    return [dict([k, float(v)] for k, v in row.items()) for row in d]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    f = 'data/wrangled.json'\n",
    "    data = read_json(f)\n",
    "    segm = unique_features('segm', data)\n",
    "    print ('classes in \"segm\" feature:')\n",
    "    print (segm)\n",
    "    \n",
    "    keys = ['sale', 'quan', 'disc', 'prof', 'segm']\n",
    "    features = sire_features(keys, data)\n",
    "    num_keys = ['sale', 'quan', 'disc', 'prof']\n",
    "    numeric_data = sire_numeric(num_keys, features)\n",
    "    \n",
    "    k, v = \"segm\", \"Home Office\"\n",
    "    m = 100\n",
    "    s_home = sire_sample(k, v, features, m)\n",
    "    \n",
    "    v = \"Consumer\"\n",
    "    s_cons = sire_sample(k, v, features, m)\n",
    "    \n",
    "    v = \"Corporate\"\n",
    "    s_corp = sire_sample(k, v, features, m)\n",
    "    print ('\\nHome Office slice:', s_home[1])\n",
    "    print('Consumer slice:', s_cons[1])\n",
    "    print ('Coporate slice:', s_corp[1])\n",
    "    print ('sample size:', m)\n",
    "    \n",
    "    df_home = pd.DataFrame(s_home[0])\n",
    "    df_cons = pd.DataFrame(s_cons[0])\n",
    "    df_corp = pd.DataFrame(s_corp[0])\n",
    "    frames = [df_home, df_cons, df_corp]\n",
    "    result = pd.concat(frames)\n",
    "    \n",
    "    plt.figure()\n",
    "    \n",
    "    parallel_coordinates(result, \n",
    "                         'segm',\n",
    "                         color = ['orange','lime','fuchsia'])\n",
    "    df = pd.DataFrame(numeric_data)\n",
    "    X = df.ix[:].values\n",
    "    X_std = StandardScaler().fit_transform(X)\n",
    "    mean_vec = np.mean(X_std, axis=0)\n",
    "    cov_mat = np.cov(X_std.T)\n",
    "    print ('\\ncovariance matrix:\\n', cov_mat)\n",
    "\n",
    "    eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
    "    print ('\\nEigenvectors:\\n', eig_vecs)\n",
    "    print ('\\nEigenvalues:\\n', np.sort(eig_vals)[::-1])\n",
    "    \n",
    "    tot = sum(eig_vals)\n",
    "    var_exp = [(i / tot)*100 for i in sorted(eig_vals,\n",
    "                                             reverse = True)]\n",
    "    print ('\\nvariance explained:\\n', var_exp)\n",
    "    \n",
    "    corr_mat = np.corrcoef(X.T)\n",
    "    print ('\\ncorrelation matrix:\\n', corr_mat)\n",
    "    \n",
    "    eig_vals, eig_vecs = np.linalg.eig(corr_mat)\n",
    "    print ('\\nEigenvectors:\\n', eig_vecs)\n",
    "    print ('\\nEigenvalues:\\n', np.sort(eig_vals)[::-1])\n",
    "    \n",
    "    tot = sum(eig_vals)\n",
    "    var_exp = [(i / tot)*100 for i in sorted(eig_vals,\n",
    "                                             reverse = True)]\n",
    "    print ('\\nvariance explained:\\n', var_exp)\n",
    "    \n",
    "    cum_var_exp = np.cumsum(var_exp)\n",
    "    fig, ax = plt.subplots()\n",
    "    labels = ['PC1', 'PC2', 'PC3', 'PC4']\n",
    "    width = 0.35\n",
    "    index = np.arange(len(var_exp))\n",
    "    ax.bar(index, var_exp,\n",
    "           color = ['fuchsia', 'lime', 'thistle', 'thistle'])\n",
    "\n",
    "    for i, v in enumerate(var_exp):\n",
    "        v = round(v, 2)\n",
    "        val = str(v) + '%'\n",
    "        ax.text(i, v+0.5, val, \n",
    "                ha = 'center', \n",
    "                color = 'b',\n",
    "                fontsize = 9,\n",
    "                fontweight = 'bold')\n",
    "    \n",
    "    plt.xticks(index, labels)\n",
    "    plt.title('Variance Explained')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speed Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, humanfriendly as hf\n",
    "from time import clock\n",
    "\n",
    "def read_json(f):\n",
    "    with open(f) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def mk_gen(k, d):\n",
    "    for row in d:\n",
    "        dic = {}\n",
    "        for key in k:\n",
    "            dic[key] = float(row[key])\n",
    "        yield dic\n",
    "\n",
    "def conv_float(keys, d):\n",
    "    return [dict([k, float(v)] for k, v in row.items()\n",
    "                 if k in keys) for row in d]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    f = 'data/wrangled.json'\n",
    "    data = read_json(f)\n",
    "    keys = ['sale', 'quan', 'disc', 'prof']\n",
    "    print ('create, convert, and display list:')\n",
    "    start = clock()\n",
    "    data = conv_float(keys, data)\n",
    "    for i, row in enumerate(data):\n",
    "        if i < 5:\n",
    "            print (row)\n",
    "    end = clock()\n",
    "    elapsed_ls = end - start\n",
    "    print (hf.format_timespan(elapsed_ls, detailed=True))\n",
    "    print ('\\ncreate, convert, and display generator:')\n",
    "    start = clock()\n",
    "    generator = mk_gen(keys, data)\n",
    "    for i, row in enumerate(generator):\n",
    "        if i < 5:\n",
    "            print (row)\n",
    "    end = clock()\n",
    "    elapsed_gen = end - start\n",
    "    print (hf.format_timespan(elapsed_gen, detailed=True))\n",
    "    speed = round(elapsed_ls / elapsed_gen, 2)\n",
    "    print ('\\ngenerator is', speed, 'times faster')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, csv\n",
    "\n",
    "def read_dat(h, f):\n",
    "    return csv.DictReader((line.replace('::', ':')\n",
    "                           for line in open(f)),\n",
    "                          delimiter=':', fieldnames=h,\n",
    "                          quoting=csv.QUOTE_NONE)\n",
    "\n",
    "def gen_dict(d):\n",
    "    for row in d:\n",
    "        yield dict(row)\n",
    "\n",
    "def dump_json(f, l, d):\n",
    "    f = open(f, 'w')\n",
    "    f.write('[')\n",
    "    for i, row in enumerate(d):\n",
    "        j = json.dumps(row)\n",
    "        f.write(j)\n",
    "        if i < l - 1:\n",
    "            f.write(',')\n",
    "        else:\n",
    "            f.write(']')\n",
    "    f.close()\n",
    "\n",
    "def read_json(f):\n",
    "    with open(f) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def display(n, f):\n",
    "    for i, row in enumerate(f):\n",
    "        if i < n:\n",
    "            print (row)\n",
    "    print()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print ('... sizing data ...\\n')\n",
    "    u_dat = 'data/ml-1m/users.dat'\n",
    "    m_dat = 'data/ml-1m/movies.dat'\n",
    "    r_dat = 'data/ml-1m/ratings.dat'\n",
    "    unames = ['user_id', 'gender', 'age', 'occupation', 'zip']\n",
    "    mnames = ['movie_id', 'title', 'genres']\n",
    "    rnames = ['user_id', 'movie_id', 'rating', 'timestamp']\n",
    "    users = read_dat(unames, u_dat)\n",
    "    ul = len(list(gen_dict(users)))\n",
    "    movies = read_dat(mnames, m_dat)\n",
    "    ml = len(list(gen_dict(movies)))\n",
    "    ratings = read_dat(rnames, r_dat)\n",
    "    rl = len(list(gen_dict(ratings)))\n",
    "    print ('size of datasets:')\n",
    "    print ('users', ul)\n",
    "    print ('movies', ml)\n",
    "    print ('ratings', rl)\n",
    "    print ('\\n... dumping data ...\\n')\n",
    "    users = read_dat(unames, u_dat)\n",
    "    users = gen_dict(users)\n",
    "    movies = read_dat(mnames, m_dat)\n",
    "    movies = gen_dict(movies)\n",
    "    ratings = read_dat(rnames, r_dat)\n",
    "    ratings = gen_dict(ratings)\n",
    "    uf = 'data/users.json'\n",
    "    dump_json(uf, ul, users)\n",
    "    mf = 'data/movies.json'\n",
    "    dump_json(mf, ml, movies)\n",
    "    rf = 'data/ratings.json'\n",
    "    dump_json(rf, rl, ratings)\n",
    "    print ('\\n... verifying data ...\\n')\n",
    "    u = read_json(uf)\n",
    "    m = read_json(mf)\n",
    "    r = read_json(rf)\n",
    "    n = 1\n",
    "    display(n, u)\n",
    "    display(n, m)\n",
    "    display(n, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, numpy as np\n",
    "\n",
    "def read_json(f):\n",
    "    with open(f) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def dump_json(f, d):\n",
    "    with open(f, 'w') as fout:\n",
    "        json.dump(d, fout)    \n",
    "\n",
    "def display(n, d):\n",
    "    [print (row) for i,row in enumerate(d) if i < n]\n",
    "\n",
    "def get_indx(k, d):\n",
    "    return [row[k] for row in d if 'null' in row]\n",
    "\n",
    "def get_data(k, l, d):\n",
    "    return [row for i, row in enumerate(d) if row[k] in l]\n",
    "\n",
    "def get_unique(key, d):\n",
    "    s = set()\n",
    "    for row in d:\n",
    "        for k, v in row.items():\n",
    "            if k in key:\n",
    "                s.add(v)\n",
    "    return np.sort(list(s))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mf = 'data/movies.json'\n",
    "    m = read_json(mf)\n",
    "    n = 20\n",
    "    display(n, m)\n",
    "    print ()\n",
    "    indx = get_indx('movie_id', m)\n",
    "    for row in m:\n",
    "        if row['movie_id'] in indx:\n",
    "            row['title'] = row['title'] + ':' + row['genres']\n",
    "            row['genres'] = row['null'][0]\n",
    "            del row['null']\n",
    "        title = row['title'].split(\" \")\n",
    "        year = title.pop()\n",
    "        year = ''.join(c for c in year if c not in '()')\n",
    "        row['title'] = ' '.join(title)\n",
    "        row['year'] = year\n",
    "    data = get_data('movie_id', indx, m)\n",
    "    n = 2\n",
    "    display(n, data)\n",
    "    s = get_unique('year', m)\n",
    "    print ('\\n', s, '\\n')\n",
    "    rec = get_data('year', ['Assignment'], m)\n",
    "    print (rec[0])\n",
    "    rec = get_data('year', [\"L'Associe1982\"], m)\n",
    "    print (rec[0], '\\n')\n",
    "    b1, b2, cnt = False, False, 0\n",
    "    for row in m:\n",
    "        if row['movie_id'] in ['1001']:\n",
    "            row['year'] = '1982'\n",
    "            print (row)\n",
    "            b1 = True\n",
    "        elif row['movie_id'] in ['2382']:\n",
    "            row['title'] = 'Police Academy 5: Assignment: Miami Beach'\n",
    "            row['genres'] = 'Comedy'\n",
    "            row['year'] = '1988'\n",
    "            print (row)\n",
    "            b2 = True\n",
    "        elif b1 and b2: break\n",
    "        cnt += 1\n",
    "    print ('\\n', cnt, len(m))\n",
    "    mf = 'data/cmovies.json'    \n",
    "    dump_json(mf, m)\n",
    "    m = read_json(mf)\n",
    "    display(n, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, numpy as np, sys, os, humanfriendly as hf\n",
    "from time import clock\n",
    "sys.path.append(os.getcwd()+'/classes')\n",
    "import conn\n",
    "\n",
    "def read_json(f):\n",
    "    with open(f) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def get_column(A, v):\n",
    "    return [A_i[v] for A_i in A]\n",
    "\n",
    "def remove_nr(v1, v2):\n",
    "    set_v1 = set(v1)\n",
    "    set_v2 = set(v2)\n",
    "    diff = list(set_v1 - set_v2)\n",
    "    return diff\n",
    "\n",
    "def get_info(*args):\n",
    "    a = [arg for arg in args]\n",
    "    ratings = [int(row[a[0][1]]) for row in a[2] if row[a[0][0]] == a[1]]\n",
    "    uids = [row[a[0][3]] for row in a[2] if row[a[0][0]] == a[1]]\n",
    "    title = [row[a[0][2]] for row in a[3] if row[a[0][0]] == a[1]]\n",
    "    age = [int(row[a[0][4]]) for col in uids for row in a[4] if col == row[a[0][3]]]\n",
    "    gender = [row[a[0][5]] for col in uids for row in users if col == row[a[0][3]]]\n",
    "    return (ratings, title[0], uids, age, gender)\n",
    "\n",
    "def generate(k, v, r, m, u):\n",
    "   for i, mid in enumerate(v):\n",
    "       dic = {}\n",
    "       rec = get_info(k, mid, r, m, u)\n",
    "       dic = {'_id':i, 'mid':mid, 'title':rec[1], 'avg_rating':np.mean(rec[0]),\n",
    "              'n_ratings':len(rec[0]), 'avg_age':np.mean(rec[3]),\n",
    "              'M':rec[4].count('M'), 'F':rec[4].count('F')}\n",
    "       dic['avg_rating'] = round(float(str(dic['avg_rating'])[:6]),2)\n",
    "       dic['avg_age'] = round(float(str(dic['avg_age'])[:6]))\n",
    "       yield dic\n",
    "\n",
    "def gen_ls(g):\n",
    "    for i, row in enumerate(g):\n",
    "        yield row\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print ('... creating datasets ...\\n')\n",
    "    m = 'data/cmovies.json'\n",
    "    movies = np.array(read_json(m))\n",
    "    r = 'data/ratings.json'\n",
    "    ratings = np.array(read_json(r))\n",
    "    r = 'data/users.json'\n",
    "    users = np.array(read_json(r))\n",
    "    print ('... creating movie indicies vector data ...\\n')\n",
    "    mv = get_column(movies, 'movie_id')\n",
    "    rv = get_column(ratings, 'movie_id')\n",
    "    print ('... creating unrated movie indicies vector ...\\n')\n",
    "    nrv = remove_nr(mv, rv)\n",
    "    diff = [int(row) for row in nrv]\n",
    "    print (np.sort(diff), '\\n')\n",
    "    new_mv = [x for x in mv if x not in nrv]\n",
    "    mid = '1'\n",
    "    keys = ('movie_id', 'rating', 'title', 'user_id', 'age', 'gender')\n",
    "    stats = get_info(keys, mid, ratings, movies, users)\n",
    "    avg_rating = np.mean(stats[0])\n",
    "    avg_age = np.mean(stats[3])\n",
    "    n_ratings = len(stats[0])\n",
    "    title = stats[1]\n",
    "    M, F = stats[4].count('M'), stats[4].count('F')\n",
    "    print ('avg rating for:', end=' \"')\n",
    "    print (title + '\" is', round(avg_rating, 2), end=' (')\n",
    "    print (n_ratings, 'ratings)\\n')\n",
    "    gen = generate(keys, new_mv, ratings, movies, users)\n",
    "    gls = gen_ls(gen)\n",
    "    obj = conn.conn('test')\n",
    "    db = obj.getDB()\n",
    "    movie_info = db.movie_info\n",
    "    movie_info.drop()\n",
    "    print ('... saving movie_info to MongoDB ...\\n')\n",
    "    start = clock()\n",
    "    for row in gls:\n",
    "        movie_info.insert(row)\n",
    "    end = clock()\n",
    "    elapsed_ls = end - start\n",
    "    print (hf.format_timespan(elapsed_ls, detailed=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, json, humanfriendly as hf\n",
    "from time import clock\n",
    "sys.path.append(os.getcwd() + '/classes')\n",
    "import conn\n",
    "\n",
    "def read_json(f):\n",
    "    with open(f) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def create_db(c, d):\n",
    "    c = db[c]\n",
    "    c.drop()\n",
    "    for i, row in enumerate(d):\n",
    "        row['_id'] = i\n",
    "        c.insert(row)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    u = read_json('data/users.json')\n",
    "    m = read_json('data/cmovies.json')\n",
    "    r = read_json('data/ratings.json')\n",
    "    obj = conn.conn('test')\n",
    "    db = obj.getDB()\n",
    "    print ('... creating MongoDB collections ...\\n')\n",
    "    start = clock()\n",
    "    create_db('users', u)\n",
    "    create_db('movies', m)\n",
    "    create_db('ratings', r)\n",
    "    end = clock()\n",
    "    elapsed_ls = end - start\n",
    "    print (hf.format_timespan(elapsed_ls, detailed=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.getcwd() + '/classes')\n",
    "import conn\n",
    "\n",
    "def match_item(k, v, d):\n",
    "    pipeline = [ {'$match' : { k : v }} ]\n",
    "    q = db.command('aggregate',d,pipeline=pipeline)\n",
    "    return q\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    obj = conn.conn('test')\n",
    "    db = obj.getDB()\n",
    "    movie = 'Toy Story'\n",
    "    q = match_item('title', movie, 'movie_info')\n",
    "    r = q['result'][0]\n",
    "    print (movie, 'document:')\n",
    "    print (r)\n",
    "    print ('average rating', r['avg_rating'], '\\n')\n",
    "    user_id = '3'\n",
    "    print ('*** user', user_id, '***')\n",
    "    q = match_item('user_id', user_id, 'users')\n",
    "    r = q['result'][0]    \n",
    "    print ('age', r['age'], 'gender', r['gender'], 'occupation',\\\n",
    "          r['occupation'], 'zip', r['zip'], '\\n')\n",
    "    print ('*** \"user 3\" movie ratings of 5 ***')\n",
    "    q = match_item('user_id', user_id, 'ratings')\n",
    "    mid = q['result']\n",
    "    for row in mid:\n",
    "        if row['rating'] == '5':\n",
    "            q = match_item('movie_id', row['movie_id'], 'movies')\n",
    "            title = q['result'][0]['title']\n",
    "            genre = q['result'][0]['genres']\n",
    "            print (row['movie_id'], title, genre)\n",
    "    mid = '1136'\n",
    "    q = match_item('mid', mid, 'movie_info')\n",
    "    title = q['result'][0]['title']\n",
    "    avg_rating = q['result'][0]['avg_rating']\n",
    "    print ()\n",
    "    print ('\"' + title + '\"', 'average rating:', avg_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.getcwd() + '/classes')\n",
    "import conn\n",
    "\n",
    "def stages(k, v, r, d):\n",
    "    pipeline = [ {'$match' : { '$and' : [ { k : v },\n",
    "                   {'rating':{'$eq':r} }] } },\n",
    "                 {'$project' : {\n",
    "                     '_id' : 1,\n",
    "                     'user_id' : 1,\n",
    "                     'movie_id' : 1,\n",
    "                     'rating' : 1 } },\n",
    "                 {'$limit' : 100}]\n",
    "    q = db.command('aggregate',d,pipeline=pipeline)\n",
    "    return q\n",
    "\n",
    "def match_item(k, v, d):\n",
    "    pipeline = [ {'$match' : { k : v }} ]\n",
    "    q = db.command('aggregate',d,pipeline=pipeline)\n",
    "    return q\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    obj = conn.conn('test')\n",
    "    db = obj.getDB()\n",
    "    u = '3'\n",
    "    r = '5'\n",
    "    q = stages('user_id', u, r, 'ratings')\n",
    "    result = q['result']\n",
    "    print ('ratings of', r, 'for user ' + str(u) + ':')\n",
    "    for i, row in enumerate(result):\n",
    "        print (row)\n",
    "    n = i+1\n",
    "    print ()\n",
    "    print (n, 'associated movie titles:')\n",
    "    for i, row in enumerate(result):\n",
    "        q = match_item('movie_id', row['movie_id'], 'movies')\n",
    "        r = q['result'][0]\n",
    "        print (r['title'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    consumer_key = 'qIhSaP3SDMjYTgQSHMnflOte0'\n",
    "    consumer_secret = '7M4HGS8iPmma0tsC6KGMhqtlR4tDBl1YWPu1UtIMT5mvRNQw35'\n",
    "    access_token = '3417237687-G6xlBPoHYQclCUhLoULgL6ubwiDjLmFUp1dEEqi'\n",
    "    access_encrypted = 'zCDKTOHNFa31nBEGOhbttbA5RWX6lw7NR5jDsVJa3d6bh'\n",
    "    data = {}\n",
    "    data['ck'] = consumer_key\n",
    "    data['cs'] = consumer_secret\n",
    "    data['at'] = access_token\n",
    "    data['ae'] = access_encrypted\n",
    "    json_data = json.dumps(data)\n",
    "    header = '[\\n'\n",
    "    ender = ']'\n",
    "    obj = open('data/credentials.json', 'w')\n",
    "    obj.write(header)\n",
    "    obj.write(json_data + '\\n')\n",
    "    obj.write(ender)\n",
    "    obj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TwitterSearch import *\n",
    "import json, sys\n",
    "\n",
    "class twitSearch:\n",
    "    def __init__(self, cred, ls, limit):\n",
    "        self.cred = cred\n",
    "        self.ls = ls\n",
    "        self.limit = limit\n",
    "    def search(self):\n",
    "        num = 0\n",
    "        dt = []\n",
    "        dic = {}\n",
    "        try:\n",
    "            tso = TwitterSearchOrder()\n",
    "            tso.set_keywords(self.ls)\n",
    "            tso.set_language('en')\n",
    "            tso.set_include_entities(False)\n",
    "            ts = TwitterSearch(\n",
    "                consumer_key = self.cred[0]['ck'],\n",
    "                consumer_secret = self.cred[0]['cs'],\n",
    "                access_token = self.cred[0]['at'],\n",
    "                access_token_secret = self.cred[0]['ae']\n",
    "                )\n",
    "            for tweet in ts.search_tweets_iterable(tso):\n",
    "                if num <= self.limit:\n",
    "                    dic['_id'] = num\n",
    "                    dic['tweeter'] = tweet['user']['screen_name']\n",
    "                    dic['tweet_text'] = tweet['text']\n",
    "                    dt.append(dic)\n",
    "                    dic = {}\n",
    "                else:\n",
    "                    break\n",
    "                num += 1\n",
    "        except TwitterSearchException as e:\n",
    "            print (e)\n",
    "        return dt\n",
    "\n",
    "def get_creds():\n",
    "    with open('data/credentials.json') as json_data:\n",
    "        d = json.load(json_data)\n",
    "        json_data.close()\n",
    "    return d\n",
    "\n",
    "def write_json(f, d):\n",
    "    with open(f, 'w') as fout:\n",
    "        json.dump(d, fout)\n",
    "\n",
    "def translate():\n",
    "    return dict.fromkeys(range(0x10000, sys.maxunicode + 1), 0xfffd)\n",
    "\n",
    "def read_json(f):\n",
    "    with open(f) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    cred = get_creds()\n",
    "    ls = ['machine', 'learning']\n",
    "    limit = 10\n",
    "    obj = twitSearch(cred, ls, limit)\n",
    "    data = obj.search()\n",
    "    f = 'data/TwitterSearch.json'\n",
    "    write_json(f, data)\n",
    "    non_bmp_map = translate()\n",
    "    print ('twitter data:')\n",
    "    for row in data:\n",
    "        row['tweet_text'] = str(row['tweet_text']).translate(non_bmp_map)\n",
    "        tweet_text = row['tweet_text'][0:50]\n",
    "        print ('{:<3}{:18s}{}'.format(row['_id'], row['tweeter'], tweet_text))\n",
    "    print ('\\nverify JSON:')\n",
    "    read_data = read_json(f)\n",
    "    for i, p in enumerate(read_data):\n",
    "        if i < 3:\n",
    "            p['tweet_text'] = str(p['tweet_text']).translate(non_bmp_map)\n",
    "            tweet_text = p['tweet_text'][0:50]\n",
    "            print ('{:<3}{:18s}{}'.format(p['_id'], p['tweeter'], tweet_text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests, json\n",
    "\n",
    "def build_title(t):\n",
    "    t = t.text\n",
    "    t = t.split()\n",
    "    ls = []\n",
    "    for row in t:\n",
    "        if row != '-':\n",
    "            ls.append(row)\n",
    "        elif row == '-':\n",
    "            break\n",
    "    return ' '.join(ls)\n",
    "\n",
    "def release_date(r):\n",
    "    r = r.text\n",
    "    r = r.split()\n",
    "    prefix = r[0] + s + r[1]\n",
    "    if len(r) == 5:\n",
    "        date = r[2] + s + r[3] + s + r[4]\n",
    "    else:\n",
    "        date = r[2] + s + r[3]\n",
    "    return prefix, date        \n",
    "\n",
    "def write_json(f, d):\n",
    "    with open(f, 'w') as fout:\n",
    "        json.dump(d, fout)\n",
    "\n",
    "def read_json(f):\n",
    "    with open(f) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    s = ' '\n",
    "    dic_ls = []\n",
    "    base_url = \"https://ssearch.oreilly.com/?q=data+science\"\n",
    "    soup = BeautifulSoup(requests.get(base_url).text, 'lxml')\n",
    "    books = soup.find_all('article')\n",
    "    for i, row in enumerate(books):\n",
    "        dic = {}\n",
    "        tag = row.name\n",
    "        tag_val = row['class']\n",
    "        title = row.find('p', {'class' : 'title'})\n",
    "        title = build_title(title)\n",
    "        url = row.find('a', {'class' : 'learn-more'})\n",
    "        learn_more = url.get('href')\n",
    "        author = row.find('p', {'class' : 'note'}).text\n",
    "        release = row.find('p', {'class' : 'note date2'})\n",
    "        prefix, date = release_date(release)\n",
    "        if len(tag_val) == 2:\n",
    "            publisher = row.find('p', {'class' : 'note publisher'}).text\n",
    "            item = row.find('img', {'class' : 'book'})\n",
    "            cat = item.get('class')[0]\n",
    "        else:\n",
    "            publisher, cat = None, None\n",
    "            desc = row.find('p', {'class' : 'description'}).text.split()\n",
    "            desc = [row for i, row in enumerate(desc) if i < 7]\n",
    "            desc = ' '.join(desc) + ' ...'\n",
    "        dic['title'] = title\n",
    "        dic['learn_more'] = learn_more\n",
    "        if author[0:3] != 'Pub':\n",
    "            dic['author'] = author\n",
    "        if publisher is not None:\n",
    "            dic['publisher'] = publisher\n",
    "            dic['category'] = cat\n",
    "        else:\n",
    "            dic['event'] = desc \n",
    "        dic['date'] = date\n",
    "        dic_ls.append(dic)\n",
    "    f = 'data/scraped.json'\n",
    "    write_json(f, dic_ls)\n",
    "    data = read_json(f)\n",
    "    for i, row in enumerate(data):\n",
    "        if i < 6:\n",
    "            print (row['title'])\n",
    "            if 'author' in row.keys():\n",
    "                print (row['author'])\n",
    "            if 'publisher' in row.keys():\n",
    "                print (row['publisher'])\n",
    "            if 'category' in row.keys():\n",
    "                print ('Category:', row['category'])\n",
    "                print ('Release Date:', row['date'])\n",
    "            if 'event' in row.keys():\n",
    "                print ('Event:', row['event'])\n",
    "                print ('Publish Date:', row['date'])\n",
    "            print ('Learn more:', row['learn_more'])\n",
    "            print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -a \"Caique Miranda\" -gu \"caiquemiranda\" -iv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
